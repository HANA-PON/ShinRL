{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d38be0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import chex\n",
    "import shinrl\n",
    "import gym\n",
    "import jax.numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11e97780",
   "metadata": {},
   "source": [
    "# Create custom ShinEnv\n",
    "\n",
    "This tutorial demonstrates how to create a custom environment.\n",
    "We are going to implement the following simple two-state MDP.\n",
    "\n",
    "![MDP](../assets/simple-mdp.png)\n",
    "\n",
    "You need to implement two classes, \n",
    "1. A config class inheriting `shinrl.EnvConfig`\n",
    "2. An env class inheriting `shinrl.ShinEnv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093819f",
   "metadata": {},
   "source": [
    "## 1. Config\n",
    "\n",
    "The config class is a dataclass inheriting `shinrl.EnvConfig`.\n",
    "It holds hyperparameters of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a2b6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class ExampleConfig(shinrl.EnvConfig):\n",
    "    dS: int = 2  # number of states\n",
    "    dA: int = 2  # number of actions\n",
    "    discount: float = 0.99  # discount factor\n",
    "    horizon: int = 3  # environment horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20550f8",
   "metadata": {},
   "source": [
    "## 2. Env\n",
    "\n",
    "The main env class must inherit `shinrl.ShinEnv`.\n",
    "The followings you need to implement (See details in [/home/rl-dqn/ShinRL-JAX/shinrl/envs/base/env.py](/home/rl-dqn/ShinRL-JAX/shinrl/envs/base/env.py)):\n",
    "* **DefaultConfig** (ClassVariable): Default configuration of the environment.\n",
    "* **dS** (property): Number of states.\n",
    "* **dA** (property): Number of actions.\n",
    "* **observation_space** (property): Gym observation space.\n",
    "* **action_space** (property): Gym observation space.\n",
    "* **init_probs** (function): A function that returns the probabilities of initial states.\n",
    "* **transition** (function): Transition function of the MDP.\n",
    "* **reward** (function): Reward function of the MDP.\n",
    "* **observation** (function): Observation function of the MDP.\n",
    "\n",
    "For continuous action space envs, you also need to implement:\n",
    "* **continuous_action** (function): A function that converts a discrete action to a continuous action.\n",
    "* **discrete_action** (function): A function that converts a continuous action to a discrete action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29b19084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleEnv(shinrl.ShinEnv):\n",
    "    DefaulatConfig = ExampleConfig\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        super().__init__(config)\n",
    "\n",
    "    @property\n",
    "    def dS(self) -> int:\n",
    "        return self.config.dS\n",
    "\n",
    "    @property\n",
    "    def dA(self) -> int:\n",
    "        return self.config.dA\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return gym.spaces.Box(low=jnp.array([0, 0]), high=jnp.array([1., 1.]))\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return gym.spaces.Discrete(self.dA)\n",
    "\n",
    "    def init_probs(self):\n",
    "        return jnp.array([1.0, 0.0])\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        next_state = jnp.array([0, 1], dtype=int)\n",
    "        all_probs = jnp.array([[[0.2, 0.8], [1.0, 0.0]], [[1.0, 0.0], [0.0, 1.0]]], dtype=float)\n",
    "        probs = all_probs[state, action]\n",
    "        return next_state, probs\n",
    "\n",
    "    def reward(self, state, action):\n",
    "        all_rews = jnp.array([[0.0, 0.0], [-1.0, 1.0]], dtype=float)\n",
    "        rew = all_rews[state, action]\n",
    "        return rew\n",
    "\n",
    "    def observation(self, state):\n",
    "        return jnp.array([state, state], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc67227",
   "metadata": {},
   "source": [
    "### env.mdp\n",
    "\n",
    "These core functions (transition, reward, etc.) are automatically converted to matrix form and stored in env.mdp.\n",
    "\n",
    "* env.mdp.**dS** (int): Number of states.\n",
    "* env.mdp.**dA** (int): Number of actions.\n",
    "* env.mdp.**obs_shape** (Shape): Observation shape.\n",
    "* env.mdp.**obs_mat** (dS x (obs_shape) Array): Observation of all the states.\n",
    "* env.mdp.**rew_mat** (dS x dA Array): Reward matrix.\n",
    "* env.mdp.**tran_mat** ((dSxdA) x dS SparseMat): Tranition matrix.\n",
    "* env.mdp.**init_probs** (dS Array): Probability of initial states.\n",
    "* env.mdp.**discount (float): Discount factor.\n",
    "\n",
    "These matrices are useful for analyzing the behavior (e.g., output of the Q-network) on the full state action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36878037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.,  0.],\n",
       "             [-1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.mdp.rew_mat  # dS x dA array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd00fe",
   "metadata": {},
   "source": [
    "# What you can do with ShinEnv\n",
    "\n",
    "Implemented ShinEnvs can be used like a usual gym.Env:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b19c9dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExampleEnv.DefaulatConfig()\n",
    "env = ExampleEnv(config)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    act = env.action_space.sample()\n",
    "    env.step(act)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba5964",
   "metadata": {},
   "source": [
    "ShinEnv also provides **oracle** methods that can compute exact quantities: \n",
    "* `calc_q` computes a Q-value table containing all possible state-action pairs given a policy.\n",
    "* `calc_optimal_q` computes the optimal Q-value table.\n",
    "* `calc_visit` calculates state visitation frequency table, for a given policy.\n",
    "* `calc_return` is a shortcut for computing exact undiscounted returns for a given policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3a96d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 1.7328961,  0.78408  ],\n",
       "             [-0.21592  ,  2.9701   ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The optimal strategy is obously taking a=0 at s=0 and a=1 at s=1.\n",
    "# You can verify that by calc_optimal_q method.\n",
    "env.calc_optimal_q()  # dS x dA array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5989a6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7599999904632568"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the optimal undiscounted return\n",
    "import distrax\n",
    "optimal_q = env.calc_optimal_q()  # dS x dA array\n",
    "optimal_policy = distrax.Greedy(optimal_q).probs\n",
    "env.calc_return(optimal_policy)  # dS x dA array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5987640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Q-values and return of an uniform policy\n",
    "key = jax.random.PRNGKey(0)\n",
    "dS, dA = env.dS, env.dA\n",
    "policy = jax.random.uniform(key, (dS, dA))\n",
    "policy = policy / policy.sum(axis=1, keepdims=True)  # dS x dA uniform policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7889ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.42350104, -0.2302916 ],\n",
       "             [-1.2302916 ,  0.5281966 ]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.calc_q(policy)  # dS x dA array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5074748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3924422264099121"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.calc_return(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbb2d76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.135055  , 0.03150126],\n",
       "             [0.1158788 , 0.05425394]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the state visitation frequency\n",
    "env.calc_visit(policy)  # dS x dA array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
